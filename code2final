import streamlit as st
import pytesseract
from pytesseract import Output
from PIL import Image
import cv2
import numpy as np
import json
import hashlib
import requests
from pdf2image import convert_from_bytes
from io import BytesIO
import os

st.title("üßæ Form Schema Extractor (PDF/Image + PyTesseract + Vision LLM API)")

# --- Secure API settings from environment variables ---
vision_llm_api_url = os.getenv("VISION_LLM_API_URL", "http://localhost:8000/api/vision")
vision_llm_user = os.getenv("VISION_LLM_USER")
vision_llm_password = os.getenv("VISION_LLM_PASSWORD")

uploaded_file = st.file_uploader("üì§ Upload a blank form image or PDF", type=["png", "jpg", "jpeg", "pdf"])

if uploaded_file is not None:
    try:
        if uploaded_file.name.endswith(".pdf"):
            pages = convert_from_bytes(uploaded_file.read())
            image = pages[0].convert("RGB")  # Only use first page for now
        else:
            image = Image.open(BytesIO(uploaded_file.getvalue())).convert("RGB")

        # Validate image size
        max_size = (2000, 2000)
        if image.size[0] > max_size[0] or image.size[1] > max_size[1]:
            image.thumbnail(max_size)
            st.info("üìê Image resized for better performance.")
        elif image.size[0] < 200 or image.size[1] < 200:
            st.warning("‚ö†Ô∏è Image may be too small for accurate OCR.")

        img_np = np.array(image)

        # --- Step 1: Preprocess Image ---
        gray = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)
        _, thresh = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY_INV)

        # --- Step 2: Generate Unique ID for the Form ---
        form_bytes = image.tobytes()
        form_hash = hashlib.md5(form_bytes).hexdigest()
        st.success(f"üìÑ Unique Form ID (MD5): {form_hash}")

        # --- Step 3: OCR with bounding boxes and context ---
        ocr_data = pytesseract.image_to_data(thresh, output_type=Output.DICT)
        n_boxes = len(ocr_data['text'])

        results = []
        for i in range(n_boxes):
            conf = int(ocr_data['conf'][i])
            text = ocr_data['text'][i].strip()
            if conf > 60 and len(text) > 1:
                x, y, w, h = ocr_data['left'][i], ocr_data['top'][i], ocr_data['width'][i], ocr_data['height'][i]
                results.append({
                    "text": text,
                    "bbox": [x, y, x + w, y + h],
                    "confidence": conf,
                    "line_num": ocr_data['line_num'][i],
                    "block_num": ocr_data['block_num'][i],
                    "page_num": ocr_data['page_num'][i],
                    "width": w,
                    "height": h
                })
                cv2.rectangle(img_np, (x, y), (x + w, y + h), (0, 255, 0), 2)

        st.image(img_np, caption="Detected text with bounding boxes", channels="RGB")

        # --- Step 4: Generate prompt for Vision LLM ---
        if results:
            st.subheader("üîç OCR Output")
            st.json(results)

            form_meta = {
                "form_id": form_hash,
                "image_width": image.width,
                "image_height": image.height,
                "num_ocr_entries": len(results)
            }

            prompt = f"""
You are a Vision LLM helping extract structured form schema from OCR data.

OCR data was extracted from a blank form template using PyTesseract. Your goal is to:
‚úÖ Identify fields that a human would be expected to fill in
‚úÖ Provide the field name (as labeled on the form)
‚úÖ Determine the data type (string, number, date, etc.)
‚úÖ Return the exact bounding box that represents the user-input area ‚Äî not the label
‚úÖ If possible, assign a logical section name (e.g., 'Member Info', 'Dependent Info', 'Signature') based on headings, titles, or spatial grouping

You are also given line numbers, block numbers, and page numbers. Use these to group related fields into logical sections or detect if the form spans multiple pages.

‚ùå Do NOT return bounding boxes for labels only (e.g., 'Patient Name')
‚ùå Do NOT include decorative text or titles
‚ùå Do NOT guess values ‚Äî only infer where the input is expected
‚ùå Exclude paragraphs or blocks of legal disclaimers, instructions, or small-font text commonly found in footers or margins

Form Metadata:
{json.dumps(form_meta, indent=2)}

Input OCR JSON:
{json.dumps(results, indent=2)}

Return your output as a JSON array of objects with these fields:
- field_name
- data_type
- bounding_box
- section_name (optional but recommended if identifiable)
"""

            st.subheader("üß† Prompt for Vision LLM")
            st.code(prompt, language="text")

            # --- Step 5: Send to Vision LLM API ---
            if st.button("üöÄ Call Vision LLM API"):
                try:
                    file_bytes = uploaded_file.getvalue()
                    files = {"image": file_bytes}
                    data = {"prompt": prompt}

                    headers = {}
                    if vision_llm_user and vision_llm_password:
                        headers['Authorization'] = f"Basic {vision_llm_user}:{vision_llm_password}"

                    with st.spinner("Calling Vision LLM..."):
                        response = requests.post(vision_llm_api_url, files=files, data=data, headers=headers)
                        response.raise_for_status()
                        st.subheader("üì¶ Vision LLM Response")
                        st.json(response.json())
                except requests.exceptions.RequestException as e:
                    st.error(f"‚ùå Vision LLM API call failed: {e}")
        else:
            st.warning("‚ö†Ô∏è No high-confidence text detected. Try a clearer image or different form.")

    except Exception as e:
        st.error(f"‚ùå Error processing file: {str(e)}")
