import streamlit as st
import pytesseract
from pytesseract import Output
from PIL import Image
import cv2
import numpy as np
import json
import hashlib
import requests
import io
from typing import List, Dict, Any

st.title("üßæ Form Schema Extractor (PyTesseract + Vision LLM API)")

# --- Configurable API settings ---
vision_llm_api_url = st.text_input("üîß Vision LLM API Endpoint", "http://localhost:8000/api/vision")
vision_llm_auth_token = st.text_input("üîê Optional Bearer Token (if needed)", type="password")

uploaded_file = st.file_uploader("üì§ Upload a blank form image", type=["png", "jpg", "jpeg"])

def preprocess_image(image: Image.Image) -> np.ndarray:
    """Preprocess image for better OCR results"""
    img_np = np.array(image)
    gray = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)
    
    # Apply denoising
    denoised = cv2.fastNlMeansDenoising(gray)
    
    # Apply adaptive threshold for better text detection
    thresh = cv2.adaptiveThreshold(
        denoised, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 11, 2
    )
    
    return thresh

def extract_ocr_data(image: np.ndarray, confidence_threshold: int = 60) -> List[Dict[str, Any]]:
    """Extract OCR data with improved filtering"""
    try:
        ocr_data = pytesseract.image_to_data(image, output_type=Output.DICT)
    except Exception as e:
        st.error(f"OCR processing failed: {e}")
        return []
    
    n_boxes = len(ocr_data['text'])
    results = []
    
    for i in range(n_boxes):
        confidence = int(ocr_data['conf'][i])
        if confidence > confidence_threshold:
            text = ocr_data['text'][i].strip()
            if text and len(text) > 1:  # Filter out single characters
                x, y, w, h = (
                    ocr_data['left'][i], 
                    ocr_data['top'][i], 
                    ocr_data['width'][i], 
                    ocr_data['height'][i]
                )
                
                # Skip very small boxes (likely noise)
                if w > 5 and h > 5:
                    results.append({
                        "text": text,
                        "bbox": [x, y, x + w, y + h],
                        "confidence": confidence,
                        "line_num": ocr_data['line_num'][i],
                        "block_num": ocr_data['block_num'][i],
                        "page_num": ocr_data['page_num'][i],
                        "width": w,
                        "height": h
                    })
    
    return results

def create_annotated_image(original_image: np.ndarray, ocr_results: List[Dict[str, Any]]) -> np.ndarray:
    """Create image with bounding boxes and confidence scores"""
    annotated_image = original_image.copy()
    
    for result in ocr_results:
        x1, y1, x2, y2 = result["bbox"]
        confidence = result["confidence"]
        
        # Color based on confidence: green (high) to red (low)
        color = (0, 255, 0) if confidence > 80 else (255, 165, 0) if confidence > 70 else (255, 0, 0)
        
        cv2.rectangle(annotated_image, (x1, y1), (x2, y2), color, 2)
        
        # Add confidence score
        cv2.putText(
            annotated_image, 
            f"{confidence}%", 
            (x1, y1 - 5), 
            cv2.FONT_HERSHEY_SIMPLEX, 
            0.5, 
            color, 
            1
        )
    
    return annotated_image

def generate_vision_llm_prompt(form_meta: Dict[str, Any], ocr_results: List[Dict[str, Any]]) -> str:
    """Generate improved prompt for Vision LLM"""
    return f"""
You are a Vision LLM helping extract structured form schema from OCR data.

OCR data was extracted from a blank form template using PyTesseract. Your goal is to:
‚úÖ Identify fields that a human would be expected to fill in
‚úÖ Provide the field name (as labeled on the form)
‚úÖ Determine the data type (string, number, date, email, phone, etc.)
‚úÖ Return the exact bounding box that represents the user-input area ‚Äî not the label
‚úÖ Assign a logical section name based on headings, titles, or spatial grouping
‚úÖ Identify field relationships (e.g., required fields, dependent fields)

Guidelines:
‚ùå Do NOT return bounding boxes for labels only (e.g., 'Patient Name')
‚ùå Do NOT include decorative text, titles, or instructions
‚ùå Do NOT guess values ‚Äî only infer where input is expected
‚ùå Exclude legal disclaimers, instructions, or footer text
‚úÖ Look for common form patterns: underlines, boxes, checkboxes, signature lines
‚úÖ Group related fields logically (Personal Info, Address, Emergency Contact, etc.)

Form Metadata:
{json.dumps(form_meta, indent=2)}

OCR Results ({len(ocr_results)} items):
{json.dumps(ocr_results, indent=2)}

Return your output as a JSON object with this structure:
{{
  "form_schema": {{
    "form_id": "{form_meta['form_id']}",
    "sections": [
      {{
        "section_name": "string",
        "fields": [
          {{
            "field_name": "string",
            "data_type": "string|number|date|email|phone|boolean|select",
            "bounding_box": [x1, y1, x2, y2],
            "required": boolean,
            "validation_rules": "string (optional)",
            "placeholder": "string (optional)"
          }}
        ]
      }}
    ]
  }}
}}
"""

def call_vision_llm_api(
    api_url: str, 
    image_bytes: bytes, 
    prompt: str, 
    auth_token: str = None
) -> Dict[str, Any]:
    """Call Vision LLM API with proper error handling"""
    headers = {}
    if auth_token:
        headers["Authorization"] = f"Bearer {auth_token}"
    
    # Prepare files and data
    files = {"image": ("form.jpg", image_bytes, "image/jpeg")}
    data = {"prompt": prompt}
    
    try:
        with st.spinner("Calling Vision LLM API..."):
            response = requests.post(
                api_url, 
                files=files, 
                data=data, 
                headers=headers,
                timeout=30  # Add timeout
            )
            response.raise_for_status()
            return response.json()
            
    except requests.exceptions.Timeout:
        st.error("‚ùå Request timed out. Please try again.")
        return None
    except requests.exceptions.ConnectionError:
        st.error("‚ùå Could not connect to Vision LLM API. Check the endpoint URL.")
        return None
    except requests.exceptions.HTTPError as e:
        st.error(f"‚ùå HTTP Error {response.status_code}: {response.text}")
        return None
    except requests.exceptions.RequestException as e:
        st.error(f"‚ùå Request failed: {e}")
        return None
    except json.JSONDecodeError:
        st.error("‚ùå Invalid JSON response from API")
        return None

if uploaded_file is not None:
    try:
        # Load and validate image
        image = Image.open(uploaded_file).convert("RGB")
        
        # Validate image size
        if image.width < 100 or image.height < 100:
            st.error("‚ùå Image too small. Please upload a larger image.")
            st.stop()
        
        if image.width * image.height > 50_000_000:  # ~50MP limit
            st.warning("‚ö†Ô∏è Large image detected. Resizing for better performance...")
            image.thumbnail((3000, 3000), Image.Resampling.LANCZOS)
        
        # Generate unique form ID
        img_bytes = io.BytesIO()
        image.save(img_bytes, format='JPEG', quality=95)
        form_hash = hashlib.md5(img_bytes.getvalue()).hexdigest()
        st.success(f"üìÑ Unique Form ID (MD5): {form_hash}")
        
        # Image preprocessing options
        st.subheader("üîß Preprocessing Options")
        col1, col2 = st.columns(2)
        
        with col1:
            confidence_threshold = st.slider("OCR Confidence Threshold", 30, 90, 60)
        with col2:
            use_preprocessing = st.checkbox("Enhanced Preprocessing", value=True)
        
        # Preprocess image
        if use_preprocessing:
            processed_image = preprocess_image(image)
        else:
            processed_image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2GRAY)
            _, processed_image = cv2.threshold(processed_image, 150, 255, cv2.THRESH_BINARY_INV)
        
        # Extract OCR data
        ocr_results = extract_ocr_data(processed_image, confidence_threshold)
        
        if not ocr_results:
            st.warning("‚ö†Ô∏è No high-confidence text detected. Try adjusting the confidence threshold or using a clearer image.")
        else:
            # Display annotated image
            annotated_img = create_annotated_image(np.array(image), ocr_results)
            st.image(annotated_img, caption=f"Detected {len(ocr_results)} text regions", channels="RGB")
            
            # Show OCR results
            with st.expander(f"üîç OCR Results ({len(ocr_results)} items)", expanded=False):
                st.json(ocr_results)
            
            # Form metadata
            form_meta = {
                "form_id": form_hash,
                "image_width": image.width,
                "image_height": image.height,
                "num_ocr_entries": len(ocr_results),
                "confidence_threshold": confidence_threshold,
                "preprocessing_enabled": use_preprocessing
            }
            
            # Generate and display prompt
            prompt = generate_vision_llm_prompt(form_meta, ocr_results)
            
            with st.expander("üß† Vision LLM Prompt", expanded=False):
                st.code(prompt, language="text")
            
            # Call Vision LLM API
            if st.button("üöÄ Extract Form Schema"):
                if not vision_llm_api_url.strip():
                    st.error("‚ùå Please provide a valid API endpoint URL.")
                else:
                    img_bytes.seek(0)  # Reset buffer position
                    response = call_vision_llm_api(
                        vision_llm_api_url, 
                        img_bytes.getvalue(), 
                        prompt, 
                        vision_llm_auth_token if vision_llm_auth_token.strip() else None
                    )
                    
                    if response:
                        st.subheader("üì¶ Form Schema Extracted")
                        st.json(response)
                        
                        # Option to download the schema
                        schema_json = json.dumps(response, indent=2)
                        st.download_button(
                            label="üíæ Download Form Schema",
                            data=schema_json,
                            file_name=f"form_schema_{form_hash[:8]}.json",
                            mime="application/json"
                        )
    
    except Exception as e:
        st.error(f"‚ùå Error processing image: {e}")
        st.exception(e)  # Show full traceback in development

# --- Sidebar with help information ---
with st.sidebar:
    st.markdown("### üìñ Help & Tips")
    st.markdown("""
    **For best results:**
    - Use high-resolution, clear images
    - Ensure good contrast between text and background
    - Avoid skewed or rotated forms
    - Test different confidence thresholds
    
    **Supported formats:** PNG, JPG, JPEG
    
    **API Requirements:**
    - Endpoint should accept multipart form data
    - Image field: 'image'
    - Prompt field: 'prompt'
    - Optional Bearer token authentication
    """)
