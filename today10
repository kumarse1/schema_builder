import streamlit as st
import pytesseract
from pytesseract import Output
from PIL import Image
import cv2
import numpy as np
import json
import hashlib
import requests
from io import BytesIO
import os
import base64
import logging
from datetime import datetime

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Check for PyMuPDF availability
try:
    import fitz  # PyMuPDF
    PYMUPDF_AVAILABLE = True
except ImportError:
    PYMUPDF_AVAILABLE = False
    st.warning("‚ö†Ô∏è PyMuPDF not available. PDF processing will be limited.")

st.set_page_config(
    page_title="Form Schema Extractor",
    page_icon="üßæ",
    layout="wide"
)

st.title("üßæ Form Schema Extractor (PDF/Image + PyTesseract + Vision LLM API)")

# --- Configuration sidebar ---
with st.sidebar:
    st.header("‚öôÔ∏è Configuration")
    
    # OCR Settings
    st.subheader("OCR Settings")
    confidence_threshold = st.slider("Confidence Threshold", 30, 95, 60, 5)
    min_text_length = st.slider("Minimum Text Length", 1, 5, 2)
    
    # Image Processing Settings
    st.subheader("Image Processing")
    max_width = st.number_input("Max Image Width", 500, 3000, 2000, 100)
    max_height = st.number_input("Max Image Height", 500, 3000, 2000, 100)
    
    # API Settings
    st.subheader("API Configuration")
    vision_llm_api_url = st.text_input(
        "Vision LLM API URL", 
        value=os.getenv("VISION_LLM_API_URL", "http://localhost:8000/api/vision")
    )
    
    # Check if credentials are available
    has_credentials = bool(os.getenv("VISION_LLM_USER") and os.getenv("VISION_LLM_PASSWORD"))
    st.info(f"API Credentials: {'‚úÖ Available' if has_credentials else '‚ùå Not configured'}")

# --- Secure API settings ---
vision_llm_user = os.getenv("VISION_LLM_USER")
vision_llm_password = os.getenv("VISION_LLM_PASSWORD")

def validate_image(image):
    """Validate image properties"""
    issues = []
    
    if image.size[0] < 200 or image.size[1] < 200:
        issues.append("Image may be too small for accurate OCR (< 200px)")
    
    if image.size[0] > 5000 or image.size[1] > 5000:
        issues.append("Image is very large and may cause performance issues")
    
    # Check if image is mostly blank
    img_array = np.array(image.convert('L'))
    if np.std(img_array) < 10:
        issues.append("Image appears to have very low contrast")
    
    return issues

def preprocess_image(image, enhance=True):
    """Enhanced image preprocessing for better OCR"""
    img_np = np.array(image)
    
    if enhance:
        # Convert to grayscale
        if len(img_np.shape) == 3:
            gray = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)
        else:
            gray = img_np
        
        # Apply different preprocessing techniques
        # Method 1: Simple thresholding
        _, thresh1 = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY)
        
        # Method 2: Adaptive thresholding
        thresh2 = cv2.adaptiveThreshold(
            gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2
        )
        
        # Method 3: Otsu's thresholding
        _, thresh3 = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        
        # Use the one with the most text detected (simple heuristic)
        methods = [
            ("Original", gray),
            ("Simple Threshold", thresh1),
            ("Adaptive Threshold", thresh2),
            ("Otsu Threshold", thresh3)
        ]
        
        best_method = methods[0]
        max_text_regions = 0
        
        for name, processed_img in methods:
            # Quick test to see which method detects more text regions
            contours, _ = cv2.findContours(
                processed_img if name != "Original" else cv2.threshold(processed_img, 127, 255, cv2.THRESH_BINARY)[1],
                cv2.RETR_EXTERNAL, 
                cv2.CHAIN_APPROX_SIMPLE
            )
            text_regions = len([c for c in contours if cv2.contourArea(c) > 50])
            
            if text_regions > max_text_regions:
                max_text_regions = text_regions
                best_method = (name, processed_img)
        
        st.info(f"üìä Best preprocessing method: {best_method[0]}")
        return best_method[1]
    else:
        # Simple grayscale conversion
        if len(img_np.shape) == 3:
            return cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)
        return img_np

def convert_pdf_to_image(pdf_bytes, page_num=0, dpi=200):
    """Convert PDF to image using PyMuPDF with better quality settings"""
    if not PYMUPDF_AVAILABLE:
        raise ImportError("PyMuPDF not available")
    
    try:
        pdf_document = fitz.open(stream=pdf_bytes, filetype="pdf")
        
        if page_num >= pdf_document.page_count:
            raise ValueError(f"Page {page_num} not found. PDF has {pdf_document.page_count} pages.")
        
        page = pdf_document[page_num]
        
        # Calculate zoom factor based on DPI
        zoom = dpi / 72.0  # 72 DPI is default
        mat = fitz.Matrix(zoom, zoom)
        
        pix = page.get_pixmap(matrix=mat)
        img_data = pix.tobytes("ppm")
        image = Image.open(BytesIO(img_data)).convert("RGB")
        
        pdf_document.close()
        return image, pdf_document.page_count
    except Exception as e:
        logger.error(f"PDF conversion error: {str(e)}")
        raise

def extract_pdf_text_with_positions(pdf_bytes, page_num=0):
    """Extract text with positions from PDF using PyMuPDF"""
    if not PYMUPDF_AVAILABLE:
        raise ImportError("PyMuPDF not available")
    
    try:
        pdf_document = fitz.open(stream=pdf_bytes, filetype="pdf")
        page = pdf_document[page_num]
        
        text_dict = page.get_text("dict")
        results = []
        
        for block_num, block in enumerate(text_dict["blocks"]):
            if "lines" in block:
                for line_num, line in enumerate(block["lines"]):
                    for span in line["spans"]:
                        text = span["text"].strip()
                        if len(text) >= min_text_length:
                            bbox = span["bbox"]
                            results.append({
                                "text": text,
                                "bbox": [int(bbox[0]), int(bbox[1]), int(bbox[2]), int(bbox[3])],
                                "confidence": 95,
                                "line_num": line_num + 1,
                                "block_num": block_num + 1,
                                "page_num": page_num + 1,
                                "width": int(bbox[2] - bbox[0]),
                                "height": int(bbox[3] - bbox[1]),
                                "font_size": span.get("size", 0),
                                "font_flags": span.get("flags", 0)
                            })
        
        pdf_document.close()
        return results, pdf_document.page_count
    except Exception as e:
        logger.error(f"PDF text extraction error: {str(e)}")
        raise

def perform_ocr(image, preprocessed_image):
    """Perform OCR with comprehensive error handling"""
    try:
        ocr_data = pytesseract.image_to_data(preprocessed_image, output_type=Output.DICT)
        n_boxes = len(ocr_data['text'])
        
        results = []
        img_np = np.array(image)
        
        for i in range(n_boxes):
            conf = int(ocr_data['conf'][i])
            text = ocr_data['text'][i].strip()
            
            if conf > confidence_threshold and len(text) >= min_text_length:
                x, y, w, h = (
                    ocr_data['left'][i], 
                    ocr_data['top'][i], 
                    ocr_data['width'][i], 
                    ocr_data['height'][i]
                )
                
                results.append({
                    "text": text,
                    "bbox": [x, y, x + w, y + h],
                    "confidence": conf,
                    "line_num": ocr_data['line_num'][i],
                    "block_num": ocr_data['block_num'][i],
                    "page_num": ocr_data['page_num'][i],
                    "width": w,
                    "height": h
                })
                
                # Draw bounding box
                cv2.rectangle(img_np, (x, y), (x + w, y + h), (0, 255, 0), 2)
                cv2.putText(img_np, f"{conf}%", (x, y-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)
        
        return results, img_np
    except Exception as e:
        logger.error(f"OCR error: {str(e)}")
        raise

def generate_enhanced_prompt(results, form_meta, include_context=True):
    """Generate an enhanced prompt for the Vision LLM"""
    context = ""
    if include_context:
        context = """
Additional Context:
- Look for common form patterns like checkboxes, underlines, brackets, or areas with labels followed by blank spaces
- Group related fields logically (e.g., personal info, address, medical history)
- Consider spatial relationships - fields close together are likely related
- Identify signature areas, date fields, and checkbox groups
- Pay attention to font sizes and formatting to distinguish labels from input areas
"""
    
    prompt = f"""
You are a Vision LLM expert at extracting structured form schemas from text extraction data.

TASK: Analyze the extracted text and identify INPUT FIELDS where humans would enter information.

REQUIREMENTS:
‚úÖ Identify fields that expect human input (not just labels)
‚úÖ Provide descriptive field names based on nearby labels
‚úÖ Determine appropriate data types: string, number, date, email, phone, boolean, etc.
‚úÖ Return bounding boxes for INPUT AREAS (where users write), not label areas
‚úÖ Group fields into logical sections when possible
‚úÖ Include confidence scores (1-10) for your field identification

EXCLUSIONS:
‚ùå Labels without corresponding input areas
‚ùå Titles, headers, and decorative text
‚ùå Legal disclaimers and fine print
‚ùå Instructions or help text
‚ùå Company logos or branding

{context}

Form Metadata:
{json.dumps(form_meta, indent=2)}

Extracted Text Data:
{json.dumps(results, indent=2)}

OUTPUT FORMAT:
Return a JSON object with this structure:
{{
  "form_analysis": {{
    "total_fields_identified": <number>,
    "processing_notes": "<any observations>",
    "form_type_guess": "<type of form if identifiable>"
  }},
  "fields": [
    {{
      "field_name": "<descriptive name>",
      "data_type": "<string|number|date|email|phone|boolean|select>",
      "bounding_box": [x1, y1, x2, y2],
      "section_name": "<logical grouping>",
      "confidence": <1-10>,
      "required": <true|false if determinable>,
      "placeholder_text": "<if any hints about expected format>"
    }}
  ]
}}
"""
    return prompt

def call_vision_llm_api(file_bytes, prompt):
    """Call the Vision LLM API with proper error handling and retries"""
    try:
        files = {"image": file_bytes}
        data = {"prompt": prompt}
        
        headers = {}
        if vision_llm_user and vision_llm_password:
            # Use basic auth
            credentials = base64.b64encode(f"{vision_llm_user}:{vision_llm_password}".encode()).decode()
            headers['Authorization'] = f"Basic {credentials}"
        
        # Add timeout and retry logic
        response = requests.post(
            vision_llm_api_url, 
            files=files, 
            data=data, 
            headers=headers,
            timeout=30
        )
        response.raise_for_status()
        return response.json()
        
    except requests.exceptions.Timeout:
        raise Exception("API request timed out (30s)")
    except requests.exceptions.ConnectionError:
        raise Exception("Failed to connect to Vision LLM API")
    except requests.exceptions.HTTPError as e:
        raise Exception(f"API returned error: {e.response.status_code} - {e.response.text}")
    except Exception as e:
        raise Exception(f"Unexpected API error: {str(e)}")

# --- Main App ---
uploaded_file = st.file_uploader(
    "üì§ Upload a blank form image or PDF", 
    type=["png", "jpg", "jpeg", "pdf"],
    help="Supported formats: PNG, JPG, JPEG, PDF. For best results, use high-resolution images with clear text."
)

if uploaded_file is not None:
    try:
        st.success(f"üìÅ File uploaded: {uploaded_file.name} ({uploaded_file.size:,} bytes)")
        
        image = None
        text_results = None
        is_pdf = uploaded_file.name.lower().endswith(".pdf")
        processing_method = ""
        
        if is_pdf:
            st.info("üìÑ PDF detected. Processing...")
            
            if PYMUPDF_AVAILABLE:
                try:
                    # Try to convert PDF to image
                    image, total_pages = convert_pdf_to_image(uploaded_file.getvalue())
                    processing_method = f"PDF converted to image (Page 1 of {total_pages})"
                    st.success(f"‚úÖ {processing_method}")
                    
                    if total_pages > 1:
                        st.info(f"üìã Note: Processing only the first page. PDF has {total_pages} pages total.")
                        
                except Exception as e:
                    st.warning(f"‚ö†Ô∏è PDF to image conversion failed: {str(e)}")
                    
                    # Fallback to direct text extraction
                    try:
                        text_results, total_pages = extract_pdf_text_with_positions(uploaded_file.getvalue())
                        processing_method = f"Direct PDF text extraction (Page 1 of {total_pages})"
                        st.success(f"‚úÖ {processing_method}")
                    except Exception as e2:
                        st.error(f"‚ùå PDF processing failed completely: {str(e2)}")
                        st.stop()
            else:
                st.error("‚ùå PDF processing requires PyMuPDF. Please install it or convert PDF to image format.")
                st.code("pip install PyMuPDF")
                st.stop()
                
        else:
            # Handle image files
            try:
                image = Image.open(BytesIO(uploaded_file.getvalue())).convert("RGB")
                processing_method = "Image file processed"
                st.success(f"‚úÖ {processing_method}")
            except Exception as e:
                st.error(f"‚ùå Failed to open image: {str(e)}")
                st.stop()

        # Process the image if we have one
        if image:
            # Validate image
            validation_issues = validate_image(image)
            if validation_issues:
                st.warning("‚ö†Ô∏è Image validation issues:")
                for issue in validation_issues:
                    st.write(f"  ‚Ä¢ {issue}")
            
            # Resize if needed
            max_size = (max_width, max_height)
            original_size = image.size
            if image.size[0] > max_size[0] or image.size[1] > max_size[1]:
                image.thumbnail(max_size, Image.Resampling.LANCZOS)
                st.info(f"üìê Image resized from {original_size} to {image.size}")

            # Show original image
            col1, col2 = st.columns(2)
            with col1:
                st.subheader("üì∑ Original Image")
                st.image(image, caption="Uploaded image", use_container_width=True)

            # Preprocess image
            with st.spinner("üîÑ Preprocessing image..."):
                preprocessed = preprocess_image(image, enhance=True)
            
            with col2:
                st.subheader("üîç Preprocessed Image")
                st.image(preprocessed, caption="Preprocessed for OCR", use_container_width=True)

            # Perform OCR
            with st.spinner("üî§ Performing OCR..."):
                text_results, annotated_image = perform_ocr(image, preprocessed)

            # Show OCR results
            st.subheader("üìù OCR Results")
            st.image(annotated_image, caption="Detected text with confidence scores", use_container_width=True)

        # Process results (from either OCR or direct PDF extraction)
        if text_results:
            st.success(f"üéØ Extracted {len(text_results)} text elements")
            
            # Generate form metadata
            timestamp = datetime.now().isoformat()
            if image:
                form_bytes = image.tobytes()
                form_hash = hashlib.md5(form_bytes).hexdigest()
                form_meta = {
                    "form_id": form_hash,
                    "processing_method": processing_method,
                    "image_width": image.width,
                    "image_height": image.height,
                    "num_text_elements": len(text_results),
                    "confidence_threshold": confidence_threshold,
                    "timestamp": timestamp
                }
            else:
                form_hash = hashlib.md5(uploaded_file.getvalue()).hexdigest()
                form_meta = {
                    "form_id": form_hash,
                    "processing_method": processing_method,
                    "source": "direct_pdf_extraction",
                    "num_text_elements": len(text_results),
                    "timestamp": timestamp
                }

            # Show extracted text data
            with st.expander("üîç View Extracted Text Data", expanded=False):
                st.json(text_results)

            # Generate enhanced prompt
            prompt = generate_enhanced_prompt(text_results, form_meta)
            
            with st.expander("üß† View Generated Prompt", expanded=False):
                st.code(prompt, language="text")

            # Call Vision LLM API
            if st.button("üöÄ Extract Form Schema", type="primary"):
                if not vision_llm_api_url:
                    st.error("‚ùå Vision LLM API URL not configured")
                else:
                    try:
                        with st.spinner("ü§ñ Calling Vision LLM API..."):
                            response = call_vision_llm_api(uploaded_file.getvalue(), prompt)
                            
                        st.success("‚úÖ Form schema extracted successfully!")
                        
                        # Display results in a nice format
                        col1, col2 = st.columns([2, 1])
                        
                        with col1:
                            st.subheader("üìã Extracted Form Schema")
                            st.json(response)
                        
                        with col2:
                            st.subheader("üìä Quick Stats")
                            if isinstance(response, dict) and "fields" in response:
                                st.metric("Total Fields", len(response["fields"]))
                                
                                # Group by section
                                sections = {}
                                for field in response["fields"]:
                                    section = field.get("section_name", "Other")
                                    sections[section] = sections.get(section, 0) + 1
                                
                                st.write("**Fields by Section:**")
                                for section, count in sections.items():
                                    st.write(f"‚Ä¢ {section}: {count}")
                        
                        # Provide download option
                        result_json = json.dumps(response, indent=2)
                        st.download_button(
                            label="üíæ Download Schema JSON",
                            data=result_json,
                            file_name=f"form_schema_{form_hash[:8]}.json",
                            mime="application/json"
                        )
                        
                    except Exception as e:
                        st.error(f"‚ùå Vision LLM API call failed: {str(e)}")
                        st.info(f"üí° Check your API configuration and ensure the service is running at: {vision_llm_api_url}")
        else:
            st.warning("‚ö†Ô∏è No text detected. Try:")
            st.write("‚Ä¢ A clearer, higher-resolution image")
            st.write("‚Ä¢ Adjusting the confidence threshold in the sidebar")
            st.write("‚Ä¢ Different image preprocessing settings")

    except Exception as e:
        st.error(f"‚ùå Unexpected error: {str(e)}")
        st.error("üí° Please try again with a different file or check the logs for more details.")
        logger.exception("Unexpected error in main processing")

# --- Footer ---
st.markdown("---")
st.markdown("""
<div style='text-align: center; color: #666;'>
    <p>üßæ Form Schema Extractor | Built with Streamlit & PyTesseract</p>
    <p><small>For best results, use high-resolution images with clear, dark text on light backgrounds.</small></p>
</div>
""", unsafe_allow_html=True)
