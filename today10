import streamlit as st
import pytesseract
from pytesseract import Output
from PIL import Image
import cv2
import numpy as np
import json
import hashlib
import requests
from io import BytesIO
import os
import base64

# Try to import fitz (PyMuPDF) as replacement for pdf2image
try:
    import fitz  # PyMuPDF - alternative to pdf2image that doesn't need poppler
    FITZ_AVAILABLE = True
except ImportError:
    FITZ_AVAILABLE = False

st.title("üßæ Form Schema Extractor (PDF/Image + PyTesseract + Vision LLM API)")

# --- Secure API settings from environment variables ---
vision_llm_api_url = os.getenv("VISION_LLM_API_URL", "http://localhost:8000/api/vision")
vision_llm_user = os.getenv("VISION_LLM_USER")
vision_llm_password = os.getenv("VISION_LLM_PASSWORD")

def convert_pdf_to_image(pdf_bytes):
    """Convert PDF to image using PyMuPDF (no poppler required)"""
    if not FITZ_AVAILABLE:
        raise ImportError("PyMuPDF not available. Install with: pip install PyMuPDF")
    
    try:
        # Open PDF from bytes
        pdf_document = fitz.open(stream=pdf_bytes, filetype="pdf")
        
        # Get first page
        page = pdf_document[0]
        
        # Convert to image (pixmap)
        pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))  # 2x zoom for better quality
        
        # Convert to PIL Image
        img_data = pix.tobytes("ppm")
        image = Image.open(BytesIO(img_data)).convert("RGB")
        
        pdf_document.close()
        return image
    except Exception as e:
        st.error(f"‚ùå Error converting PDF: {str(e)}")
        return None

def fallback_pdf_text_extraction(pdf_bytes):
    """Fallback: Extract text directly from PDF without image conversion"""
    if not FITZ_AVAILABLE:
        raise ImportError("PyMuPDF not available. Install with: pip install PyMuPDF")
        
    try:
        pdf_document = fitz.open(stream=pdf_bytes, filetype="pdf")
        page = pdf_document[0]
        
        # Extract text with position information
        text_dict = page.get_text("dict")
        
        results = []
        for block in text_dict["blocks"]:
            if "lines" in block:
                for line in block["lines"]:
                    for span in line["spans"]:
                        text = span["text"].strip()
                        if len(text) > 1:
                            bbox = span["bbox"]  # [x0, y0, x1, y1]
                            results.append({
                                "text": text,
                                "bbox": [int(bbox[0]), int(bbox[1]), int(bbox[2]), int(bbox[3])],
                                "confidence": 95,  # High confidence for direct PDF text
                                "line_num": 1,
                                "block_num": 1,
                                "page_num": 1,
                                "width": int(bbox[2] - bbox[0]),
                                "height": int(bbox[3] - bbox[1])
                            })
        
        pdf_document.close()
        return results, None  # No image available
    except Exception as e:
        st.error(f"‚ùå Error extracting text from PDF: {str(e)}")
        return None, None

uploaded_file = st.file_uploader("üì§ Upload a blank form image or PDF", type=["png", "jpg", "jpeg", "pdf"])

if uploaded_file is not None:
    try:
        image = None
        ocr_results = None
        is_pdf = uploaded_file.name.endswith(".pdf")
        
        if is_pdf:
            if not FITZ_AVAILABLE:
                st.error("‚ùå PDF processing requires PyMuPDF. Please install it:")
                st.code("pip install PyMuPDF")
                st.error("üí° Or convert your PDF to PNG/JPG format and re-upload.")
                st.stop()
                
            st.info("üìÑ PDF detected. Attempting to process...")
            
            # Try PyMuPDF first (Method 1)
            try:
                image = convert_pdf_to_image(uploaded_file.getvalue())
                if image:
                    st.success("‚úÖ PDF converted to image using PyMuPDF")
                else:
                    raise Exception("PyMuPDF conversion failed")
            except Exception as e:
                st.warning(f"‚ö†Ô∏è PyMuPDF conversion failed: {str(e)}")
                
                # Fallback: Direct text extraction (Method 2)
                st.info("üîÑ Trying direct PDF text extraction...")
                ocr_results, _ = fallback_pdf_text_extraction(uploaded_file.getvalue())
                
                if ocr_results:
                    st.success("‚úÖ Text extracted directly from PDF")
                else:
                    # Final fallback: Skip PDF processing (Method 3)
                    st.error("‚ùå Unable to process PDF. Please convert to image format (PNG/JPG) and re-upload.")
                    st.stop()
        else:
            # Handle regular images
            image = Image.open(BytesIO(uploaded_file.getvalue())).convert("RGB")

        # Process image if available
        if image:
            # Validate image size
            max_size = (2000, 2000)
            if image.size[0] > max_size[0] or image.size[1] > max_size[1]:
                image.thumbnail(max_size, Image.Resampling.LANCZOS)
                st.info("üìê Image resized for better performance.")
            elif image.size[0] < 200 or image.size[1] < 200:
                st.warning("‚ö†Ô∏è Image may be too small for accurate OCR.")

            img_np = np.array(image)

            # --- Step 1: Preprocess Image ---
            gray = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)
            _, thresh = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY_INV)

            # --- Step 2: Generate Unique ID for the Form ---
            form_bytes = image.tobytes()
            form_hash = hashlib.md5(form_bytes).hexdigest()
            st.success(f"üìÑ Unique Form ID (MD5): {form_hash}")

            # --- Step 3: OCR with bounding boxes and context ---
            ocr_data = pytesseract.image_to_data(thresh, output_type=Output.DICT)
            n_boxes = len(ocr_data['text'])

            ocr_results = []
            for i in range(n_boxes):
                conf = int(ocr_data['conf'][i])
                text = ocr_data['text'][i].strip()
                if conf > 60 and len(text) > 1:
                    x, y, w, h = ocr_data['left'][i], ocr_data['top'][i], ocr_data['width'][i], ocr_data['height'][i]
                    ocr_results.append({
                        "text": text,
                        "bbox": [x, y, x + w, y + h],
                        "confidence": conf,
                        "line_num": ocr_data['line_num'][i],
                        "block_num": ocr_data['block_num'][i],
                        "page_num": ocr_data['page_num'][i],
                        "width": w,
                        "height": h
                    })
                    cv2.rectangle(img_np, (x, y), (x + w, y + h), (0, 255, 0), 2)

            st.image(img_np, caption="Detected text with bounding boxes", channels="RGB")

        # Process results (whether from OCR or direct PDF extraction)
        if ocr_results:
            st.subheader("üîç Text Extraction Results")
            st.json(ocr_results)

            # Generate form metadata
            if image:
                form_bytes = image.tobytes()
                form_hash = hashlib.md5(form_bytes).hexdigest()
                form_meta = {
                    "form_id": form_hash,
                    "image_width": image.width,
                    "image_height": image.height,
                    "num_ocr_entries": len(ocr_results)
                }
            else:
                # For direct PDF text extraction
                form_hash = hashlib.md5(uploaded_file.getvalue()).hexdigest()
                form_meta = {
                    "form_id": form_hash,
                    "source": "direct_pdf_extraction",
                    "num_text_entries": len(ocr_results)
                }

            prompt = f"""
You are a Vision LLM helping extract structured form schema from text extraction data.

Text data was extracted from a blank form template. Your goal is to:
‚úÖ Identify fields that a human would be expected to fill in
‚úÖ Provide the field name (as labeled on the form)
‚úÖ Determine the data type (string, number, date, etc.)
‚úÖ Return the exact bounding box that represents the user-input area ‚Äî not the label
‚úÖ If possible, assign a logical section name (e.g., 'Member Info', 'Dependent Info', 'Signature') based on headings, titles, or spatial grouping

You are also given line numbers, block numbers, and page numbers. Use these to group related fields into logical sections or detect if the form spans multiple pages.

‚ùå Do NOT return bounding boxes for labels only (e.g., 'Patient Name')
‚ùå Do NOT include decorative text or titles
‚ùå Do NOT guess values ‚Äî only infer where the input is expected
‚ùå Exclude paragraphs or blocks of legal disclaimers, instructions, or small-font text commonly found in footers or margins

Form Metadata:
{json.dumps(form_meta, indent=2)}

Input Text Extraction JSON:
{json.dumps(ocr_results, indent=2)}

Return your output as a JSON array of objects with these fields:
- field_name
- data_type
- bounding_box
- section_name (optional but recommended if identifiable)
"""

            st.subheader("üß† Prompt for Vision LLM")
            st.code(prompt, language="text")

            # --- Step 5: Send to Vision LLM API ---
            if st.button("üöÄ Call Vision LLM API"):
                try:
                    file_bytes = uploaded_file.getvalue()
                    files = {"image": file_bytes}
                    data = {"prompt": prompt}

                    headers = {}
                    if vision_llm_user and vision_llm_password:
                        # Fix: Proper basic auth encoding
                        credentials = base64.b64encode(f"{vision_llm_user}:{vision_llm_password}".encode()).decode()
                        headers['Authorization'] = f"Basic {credentials}"

                    with st.spinner("Calling Vision LLM..."):
                        response = requests.post(vision_llm_api_url, files=files, data=data, headers=headers, timeout=30)
                        response.raise_for_status()
                        st.subheader("üì¶ Vision LLM Response")
                        st.json(response.json())
                except requests.exceptions.RequestException as e:
                    st.error(f"‚ùå Vision LLM API call failed: {e}")
        else:
            st.warning("‚ö†Ô∏è No text detected. Try a clearer image or different form.")

    except Exception as e:
        st.error(f"‚ùå Error processing file: {str(e)}")
        st.error("üí° If this is a PDF, try converting it to PNG/JPG format first.")
